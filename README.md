# IR Final Project

![Framework](images/framework.png)

### Dataset 
1. Fashion-iq: https://disk.yandex.com/d/Z2E54WCwvrQA3A

reference: https://github.com/XiaoxiaoGuo/fashion-iq/issues/18

### Modify
    step 1:
    NUM_CAPTION 15 -> 5
    BLIP2_MODEL opt6.7b -> opt2.7b

    step 3:
    get reference_name list

## 2025/05/21
### Duplicate the experiment
Run Step 1. and Step 2. as usual:
```
python src/multi_caption generator_{circo/cirr/fashioniq}.py
python src/LLM-based_editing_reasoner_{circo/cirr/fashioniq}.py
```

Step 2. will generate
```
new.cap.dress.val.json
new.cap.shirt.val.json
new.cap.toptee.val.json
```
containing the edited captions generated by an LLM. Create a new directory named `<directory_name_you_like>`, copy those jsons into
`<directory_name_you_like>/captions`, and rename them as
```
cap.dress.val.json
cap.shirt.val.json
cap.toptee.val.json
```

Then, link the `image_splits` and the `images` directory in `<directory_name_you_like>`. The final structure of the directory `<directory_name_you_like>` should be
```
<directory_name_you_like>
|-- captions
    |-- cap.dress.val.json
    |-- cap.shirt.val.json
    |-- cap.toptee.val.json
| -- image_splits
| -- images
```
The directory `FashionIQ_multi_opt_gpt35_5` is an example of such a directory. The json files in its `captions` directory are obtained by using GPT-3.5 turbo and 5 multi captions in steps 1 and 2. 

To do Step 3, run
```
./run_sem.sh
```
In the shell script, the `--dataset_path` argument should be `<directory_name_you_like>`.

## 2025/05/28

### Update
- Step 1:
    - Change to use BLIP2_MODEL opt6.7b (reacll ⭡)
    - Change to NUM_CAPTION 15 (stpe 2 spend time ⭡, reacll unkown)

- Step 2:
    - code modify input gpt prompt (reacll ⭡)

- Step 3:  
    - Modify get ref_name_list in step 3 code (reacll ⭡)
    - Add original input caption features (reacll ⭡)


### Code Explanation
    Step 2:

    LLM-based_editing_reasoner_fashioniq_v1.py -> use gpt version
    LLM-based_editing_reasoner_fashioniq.py -> use LLama2-7b version

    Step 3:
    semantic_editing_search_v1.py -> modify version

### Captions store
The captions output use BLIP2_MODEL opt6.7b run after step 1 
```
LLM-edit
| -- captions_num_5
| -- captions_num_15
| -- edit_5
```

### Captions Directory for step 3
I put the all version caption data in FashionIQ_multi_opt_gpt35_5 directory
```
FashionIQ_multi_opt_gpt35_5
| -- captions
    |-- cap.dress.val.json
    |-- cap.shirt.val.json
    |-- cap.toptee.val.json
| -- caption_gpt3.5_v1 -> gpt 
| -- caption_Llama2 -> Llama2-7b
| -- image_splits
| -- images
```

### Ablation Study Results (FashionIQ Recall)

#### LLM: LLama2-7B captions-num=5
| Category | Model | pos_factor | neg_factor | add_original| Recall@10 (%) | Recall@50(%) |
|----|---|------|---|---|--------|--------|
| Shirt | SEIZE-g | 0.5 | 0 | ✔️ | 33.32 | 52.60 |
| Dress | SEIZE-g | 0.5 | 0 | ✔️ | 28.06 | 50.27 |
| Toptee | SEIZE-g | 0.5 | 0 | ✔️ | 37.48 | 59.15 |
| **Average** | | | | **32.95** | **54.01** |
| | | | | | | |
| Shirt | SEIZE-G | 0.5 | 0 | ✔️ | 34.59 | 52.75 |
| Dress | SEIZE-G | 0.5 | 0 | ✔️ | 26.82 | 48.83 |
| Toptee | SEIZE-G | 0.5 | 0 | ✔️ | 37.43 | 60.22 |
| **Average** | | | | **32.95** | **53.94** |
| | | | | | | |
| Shirt | SEIZE-H | 0.5 | 0 | ✔️ | 32.58 | 51.28 |
| Dress | SEIZE-H | 0.5 | 0 | ✔️ | 27.66 | 49.43 |
| Toptee | SEIZE-H | 0.5 | 0 | ✔️ | 35.95 | 60.28 |
| **Average** | | | | **32.07** | **53.66** |
| | | | | | | |
| Shirt | SEIZE-H | 0.5 | 0 | ✔️ | 32.58 | 51.28 |
| Dress | SEIZE-H | 0.5 | 0 | ✔️ | 27.66 | 49.43 |
| Toptee | SEIZE-H | 0.5 | 0 | ✔️ | 35.95 | 60.28 |
| **Average** | | | | **32.07** | **53.66** |
| | | | | | | |
| Shirt | SEIZE-L | 0.5 | 0 | ❌ | 25.42 | 41.12 |
| Dress | SEIZE-L | 0.5 | 0 | ❌ | 18.00 | 39.12 |
| Toptee | SEIZE-L | 0.5 | 0 | ❌ | 25.04 | 47.12 |
| **Average** | | | | **22.82** | **42.45** |
| | | | | | | |
| Shirt | SEIZE-g | default | default | ❌ | 33.91 | 51.37|
| Dress | SEIZE-g | default | default | ❌ | 26.62 | 47.00 |
| Toptee | SEIZE-g | default | default | ❌ | 34.83 | 56.40 |
| **Average** | | | | **31.79** | **51.59** |


#### LLM: GPT3.5 captions-num=5
| Category | Model | pos_factor | neg_factor | add_original| Recall@10 (%) | Recall@50(%) |
|----|---|------|---|---|--------|--------|
| Shirt | SEIZE-g | 0.5 | 0 | ❌ |  |  |
| Dress | SEIZE-g | 0.5 | 0 | ❌ | 31.38 | 53.74 |
| Toptee | SEIZE-g | 0.5 | 0 | ❌ |  |  |
| **Average** | | | | |  |
| | | | | | | |
| Shirt | SEIZE-g | 0.5 | 0 | ✔️ |  |  |
| Dress | SEIZE-g | 0.5 | 0 | ✔️ | 30.64 | 52.80 |
| Toptee | SEIZE-g | 0.5 | 0 | ✔️ |  |  |
| **Average** | | | | |  |
| | | | | | | |
| Shirt | SEIZE-g | default | default  | ❌ |  |  |
| Dress | SEIZE-g | default  | default  | ❌ | 30.59 | 53.40 |
| Toptee | SEIZE-g | default  | default  | ❌ |  |  |

-------------------------------------------
**NEW Update**

#### LLM: LLama2-7B captions-num=15
| Category | Model | pos_factor | neg_factor | add_original| Recall@10 (%) | Recall@50(%) |
|----|---|------|---|---|--------|--------|
| Shirt | SEIZE-g | 0.5 | 0 | ✔️ | 33.42 | 53.19 |
| Dress | SEIZE-g | 0.5 | 0 | ✔️ | 28.16 | 50.47 |
| Toptee | SEIZE-g | 0.5 | 0 | ✔️ | 37.17 | 59.77 |
| **Average** | | | | **32.92** | **54.48** |
| | | | | | | |
| Shirt | SEIZE-g | 0.5 | 0 | ❌ | 34.30 | 54.02 |
| Dress | SEIZE-g | 0.5 | 0 | ❌ | 27.37 | 49.63 |
| Toptee | SEIZE-g | 0.5 | 0 | ❌ | 37.17 | 59.20 |
| **Average** | | | | **32.95** | **54.29** |
| | | | | | | |
| Shirt | SEIZE-g | default | default | ❌ | 34.54 | 53.73 |
| Dress | SEIZE-g | default | default | ❌ | 27.37 | 48.88 |
| Toptee | SEIZE-g | default | default | ❌ | 36.87 | 58.13 |
| **Average** | | | | **32.93** | **53.58** |
| | | | | | | |
| Shirt | SEIZE-G | default | default | ❌ | 35.72 | 54.37 |
| Dress | SEIZE-G | default | default | ❌ | 26.77 | 47.89 |
| Toptee | SEIZE-G | default | default | ❌ | 37.63 | 59.15 |
| **Average** | | | | **33.38** | **53.80** |


#### LLM: GPT3.5 captions-num=15

config: alpha=0.8, beta=0.25

| Category | Model | pos_factor | neg_factor | add_original| Recall@10 (%) | Recall@50(%) |
|----|---|------|---|---|--------|--------|
| Shirt | SEIZE-G | default | default | ❌ | 40.38 | 58.54 |
| Dress | SEIZE-G | default | default | ❌ | 31.73 | 54.73 |
| Toptee | SEIZE-G | default | default | ❌ | 40.39 | 63.84 |
| **Average** | | | | | **37.50** | **59.04** |
| | | | | | | |
| Shirt | SEIZE-G | default | default | ✔️ | 40.38 | 58.00 |
| Dress | SEIZE-G | default | default | ✔️ | 31.83 | 55.13 |
| Toptee | SEIZE-G | default | default | ✔️ | 40.80 | 64.41 |
| **Average** | | | | | **37.67** | **59.18** |
| | | | | | | |
| Shirt | SEIZE-G | 0.5 | 0 | ❌ | 40.53 | 59.52 |
| Dress | SEIZE-G | 0.5 | 0 | ❌ | 32.47 | 55.78 |
| Toptee | SEIZE-G | 0.5 | 0 | ❌ | 40.95 | 64.56 |
| **Average** | | | | | **37.98** | **59.95** |
| | | | | | | |
| Shirt | SEIZE-G | 0.5 | 0 | ✔️ | 40.09 | 59.13 |
| Dress | SEIZE-G | 0.5 | 0 | ✔️ | 32.42 | 55.83 |
| Toptee | SEIZE-G | 0.5 | 0 | ✔️ | 41.51 | 65.37 |
| **Average** | | | | | **38.01** | **60.11** |
| | | | | | | |
| Shirt | SEIZE-g | default | default | ❌ | 38.22 | 57.51 |
| Dress | SEIZE-g | default | default | ❌ | 31.43 | 54.93 |
| Toptee | SEIZE-g | default | default | ❌ | 40.49 | 62.88 |
| **Average** | | | | | **36.72** | **58.44** |
| | | | | | | |
| Shirt | SEIZE-g | default | default | ✔️ | 39.01 | 57.61 |
| Dress | SEIZE-g | default | default | ✔️ | 31.48 | 55.53 |
| Toptee | SEIZE-g | default | default | ✔️ | 41.25 | 63.28 |
| **Average** | | | | | **37.25** | **58.81** |

| Category | Model | pos_factor | neg_factor | add_original| text_feature |Recall@10 (%) | Recall@50(%) |
|----|---|------|---|---|--|------|--------|
| Shirt | SEIZE-G | default | default | ✔️ | ❌ | 40.38 | 58.00 |
| Dress | SEIZE-G | default | default | ✔️ | ❌ | 31.83 | 55.13 |
| Toptee | SEIZE-G | default | default | ✔️ | ❌ |40.80 | 64.41 |
| **Average** | | | | | | **37.67** | **59.18** |
| | | | | | | | |
| Shirt | SEIZE-G | default | default | ✔️ | ✔️ | 40.73 | 58.34 |
| Dress | SEIZE-G | default | default | ✔️ | ✔️ | 31.68 | 55.13 |
| Toptee | SEIZE-G | default | default | ✔️ | ✔️ |42.07 | 64.71 |
| **Average** | | | | | | **38.16** | **59.39** |
| | | | | | | | |
| Shirt | SEIZE-G | 0.5 | 0 | ✔️ | ❌ | 40.09 | 59.13 |
| Dress | SEIZE-G | 0.5 | 0 | ✔️ | ❌ | 32.42 | 55.83 |
| Toptee | SEIZE-G | 0.5 | 0 | ✔️ | ❌ |41.51 | 65.37 |
| **Average** | | | | | | **38.01** | **60.11** |
| | | | | | | | |
| Shirt | SEIZE-G | 0.5 | 0 | ✔️ | ✔️ | 40.28 | 59.13 |
| Dress | SEIZE-G | 0.5 | 0 | ✔️ | ✔️ | 32.23 | 56.07 |
| Toptee | SEIZE-G | 0.5 | 0 | ✔️ | ✔️ |42.43 | 65.17 |
| **Average** | | | | | | **38.31** | **60.12** |

### Hyperparamter Analysis

- Hyperparmeter define:

        alpha -> add_original hyperparameter
        beta  -> text_feature hyperparameter

Analysis by GPT caption_num=15, defalut pos_fator & neg factor, model=SEIZE-G

#### Recall@10(%)
|  Alpha  |   Shirt   |   Dress   |   Toptee  |  Average  |
| :-----: | :-------: | :-------: | :-------: | :-------: |
|   0.2   |   40.38   | **32.03** |   40.54   |   37.65   |
|   0.3   | **40.53** |   31.98   |   40.64   |   37.72   |
|   0.4   |   40.48   |   31.88   |   40.59   |   37.65   |
|   0.5   |   40.43   |   31.93   |   40.74   |   37.70   |
|   0.6   |   40.38   |   31.73   |   40.95   |   37.69   |
|   0.7   |   40.33   |   31.88   |   40.95   |   37.72   |
| **0.8** |   40.38   |   31.83   |   40.80   |   37.67   |
|   0.9   |   40.24   |   31.68   |   40.90   |   37.60   |
|   1.0   |   40.24   |   31.73   | **41.51** | **37.83** |
#### Recall@50(%)
|  Alpha  |   Shirt   |   Dress   |   Toptee  |  Average  |
| :-----: | :-------: | :-------: | :-------: | :-------: |
|   0.2   |   58.39   |   54.98   |   64.05   |   59.14   |
|   0.3   | **58.49** |   55.03   |   64.15   |   59.22   |
|   0.4   |   58.39   |   55.03   |   64.05   |   59.16   |
|   0.5   |   58.24   |   55.03   |   64.30   |   59.19   |
|   0.6   |   58.15   |   55.03   |   64.46   |   59.21   |
|   0.7   |   58.10   |   54.98   |   64.51   |   59.20   |
| **0.8** |   58.00   |   55.13   |   64.41   |   59.18   |
|   0.9   |   58.15   | **55.28** | **64.71** | **59.38** |
|   1.0   |   58.19   |   55.13   |   64.66   |   59.33   |

Analysis by GPT caption_num=15, defalut pos_fator & neg factor, model=SEIZE-G, alpha=0.8

#### Recall@10(%)
|   Beta   |   Shirt   |   Dress   |   Toptee  |  Average  |
| :------: | :-------: | :-------: | :-------: | :-------: |
|   0.1    |   40.33   | **32.03** |   41.51   |   37.96   |
|   0.15   |   40.28   |   31.98   |   41.76   |   38.01   |
|   0.2    |   40.43   |   31.68   |   41.97   |   38.03   |
| **0.25** | **40.73** |   31.68   |   42.07   | **38.16** |
|   0.3    |   40.48   |   31.63   | **42.33** |   38.15   |
|   0.4    |   40.24   |   31.48   |   41.12   |   37.95   |

#### Recall@50(%)
|   Beta   |   Shirt   |   Dress   |   Toptee  |  Average  |
| :------: | :-------: | :-------: | :-------: | :-------: |
|   0.1    |   58.29   | **55.38** |   64.81   | **59.50** |
|   0.15   |   58.39   |   55.13   | **64.86** |   59.46   |
|   0.2    | **58.39** |   54.93   |   64.71   |   59.35   |
| **0.25** |   58.34   |   55.13   |   64.71   |   59.39   |
|   0.3    |   58.10   |   54.98   |   64.56   |   59.21   |
|   0.4    |   58.29   |   54.93   |   64.30   |   59.18   |


## 2025/06/03
### Updates
In `src/semantic_editing_search_v1.py`, 
- I added a `use_cache=False` argument in `fiq_compute_val_metrics` function so that it does not load the cached `predicted_features`, `original_features`, `target_names`, etc but recompute them instead. This is to prevent the possiblity that the cache is wrong. For example, I am computing `predicted_features` from a relative dataset of the same dress type and model type, but the dataset uses GPT outputs from prompt 2, whereas the cache stores predicted_features from GPT outputs from prompt 1. Then the cached `predicted_features` should not be used here. Thus, it is recommended to set `use_cache=False`.

- The line `os.environ['CUDA_VISIBLE_DEVICES'] = '1'` is commented out.

- The variables `distances` and `sorted_index_names` are saved. These variables are later needed for visualization. So far, they are stored in the directory `retrieved_index_names`, but the save location is subject to change. 

In `src/utils.py`,
- Added `sample_dataset` to create a sample of dataset json. This is useful when one wants to test something with the dataset quickly but does not want to wait for running through the whole dataset. 


NEW
- Framework picture
- Step 3: add compare image database captions text feature with edited caption features
- Gpt captions_num = 15 result

## 2025/06/04
### Updates
The updates are mainly in `src/semantic_editing_search_v1.py`. The main function will additionally create an `output` directory storing extra experiment outputs. Each running experiment will create a directory named by the running datetime under `output`. Such a directory stores 
- `args.json` containing experiment input arguments for reproducibility.
- `retrieved_index_names` containing the retrieved image index names and the retrieved images' distance matrices. They are used for visualization. 

The updates above for the main loop mainly adds things and do not modify existing code, so don't worry that it would change the logic of the main loop.

Running the python script stays the same, that is,
```
python src/semantic_editing_search_v1.py <ARGS>
```

## Visualization
A method `visualize` in `src/semantic_editing_search_v1.py` is added to visualize the retrieved images in an experiment. This is useful for demos and seeing where the retrieval can be improved. 

To use it, comment out `main()` under the `if __name__ == "__main__` block and write
```
visualize(experiment_output_dir, item_idx, dress_type)
```
where 
- `experiment_output_dir` is a directory under, for example, `FashionIQ_cap_num_15_split1/outputs`, that stores the `retrieved_index_names` and `args.json` of a previous experiment. 
- `item_idx` is the index of a piece of data in the relative FashionIQDataset specified in the experiment's dataset_path argument. For example, if you want to see the inputs, ground truth image, and the retrieved images of the first input, set `item_idx` to be 0.
- `dress_type`: dress, shirt, or toptee. 

Then, as usual, run
```
python src/semantic_editing_search_v1.py <ARGS>
```
Note that `<ARGS>` must be the same as the args of the experiment. I enforce that the two args must be the same when running `visualize`, meaning that an exception will be raised if they are not the same. For reasons why, see comments in the code.

## Visualization Results
Han-Yuan ran an experiment using the captions in `FashionIQ_cap_num_15_split1` and the arguments
```
{
    "dataset": "fashioniq",
    "dataset_path": "FashionIQ_cap_num_15_split1",
    "model_type": "SEIZE-G",
    "gpt_version": "gpt-3.5",
    "submission_name": "cap_num_15_split1",
    "caption_type": "opt",
    "nums_caption": 7,
    "use_momentum_strategy": true,
    "pos_factor": 0.13,
    "neg_factor": 2.1,
}
```
Then, he used `visualize` to visualize the retrieved images given by that experiment. Here are his comments on a few visualization cases:

### item_idx=0, dress_type='shirt'
Relative Captions: 'is solid white', 'is a lighter color'

Problem: relative captions suggest the style should be solid white AND a lighter color. The gpt-generated captions seem to use OR instead of AND, so the gpt-generated captions include "a young man is wearing a light blue shirt" and "a young man is posing with a light gray tee shirt on". So the retrieved images contain non-white color

### item_idx=2000, dress_type='shirt'
Relative Captions: ' is a shirt and has a dotted pattern', 'has more white and wider checks'

The target image does not seem to have dotted patterns?

### item_idx=2001, dress_type='shirt'
relative_captions: 'is a white long sleeved shirt', 'is solid white in color'

Too many irrelevant information in generated captions: on a skateboard, holding a tennis racket.

Top retrieved images are not of the texture of a shirt. The gpt-generated captions do have the word "shirt" though, so is it the CLIP embedding's problem?

### item_idx=1000, dress_type='shirt'
relative_captions: ['The shirt is black with a skeleton.', ' is red']
multi_opt: ['the umojm logo t shirt', 'the only utah map t shirt', 'the black shirt with white print, with an image of a map of the world', 'a t - shirt with the words "the only world movement is not to play"', 'a black shirt that says only the unique is moving not to play', 'an image of a t - shirt with an image of a map and the world', 'the shirt for a band called the ultimate mod movement, it has a map of the world']
multi_gpt_opt: ['The Umojm logo t-shirt is black with a red skeleton design.', 'a black t-shirt with a skeleton, only Utah map, is red.', 'a black shirt with a red skeleton print.', 'a black t-shirt with a skeleton and red text "the only winning move is not to play"', 'a black shirt with a red skeleton that says "only the unique is moving not to play"', 'a black t-shirt with a red map and world image on it.', 'The shirt for a band called the ultimate mod movement, it has a map of the world, and the skeleton is red.']
captions: ['The shirt is black with a skeleton.', ' is red']

NEW
- Captions: I update the captions result from step 2 for captions_num=15
- Captions_{DRESS}.text: for images database caption text

```
FashionIQ_multi_opt_gpt35_5
| -- captions -> step 3 load captions
    |-- cap.dress.val.json
    |-- cap.shirt.val.json
    |-- cap.toptee.val.json
| -- caption_gpt_15     -> gpt3.5, caption_num=15
| -- caption_gpt3.5_v1  -> gpt3.5, caption_num=5
| -- caption_Llama2     -> Llama2-7b, , caption_num=5
| -- caption_Llama2_15  -> Llama2-7b, , caption_num=5
| -- image_splits
| -- captions_dress.text
| -- captions_shirt.text
| -- captions_toptee.text
| -- images
```

# Citatiion
```
@inproceedings{yang2024semantic,
  title={Semantic Editing Increment Benefits Zero-Shot Composed Image Retrieval},
  author={Yang, Zhenyu and Qian, Shengsheng and Xue, Dizhan and Wu, Jiahong and Yang, Fan and Dong, Weiming and Xu, Changsheng},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={1245--1254},
  year={2024}
}
```