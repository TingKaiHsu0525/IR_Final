# IR Final Project

### Dataset 
1. Fashion-iq: https://disk.yandex.com/d/Z2E54WCwvrQA3A

reference: https://github.com/XiaoxiaoGuo/fashion-iq/issues/18

### Modify
    step 1:
    NUM_CAPTION 15 -> 5
    BLIP2_MODEL opt6.7b -> opt2.7b

    step 3:
    get reference_name list

## 2025/05/21
### Duplicate the experiment
Run Step 1. and Step 2. as usual:
```
python src/multi_caption generator_{circo/cirr/fashioniq}.py
python src/LLM-based_editing_reasoner_{circo/cirr/fashioniq}.py
```

Step 2. will generate
```
new.cap.dress.val.json
new.cap.shirt.val.json
new.cap.toptee.val.json
```
containing the edited captions generated by an LLM. Create a new directory named `<directory_name_you_like>`, copy those jsons into
`<directory_name_you_like>/captions`, and rename them as
```
cap.dress.val.json
cap.shirt.val.json
cap.toptee.val.json
```

Then, link the `image_splits` and the `images` directory in `<directory_name_you_like>`. The final structure of the directory `<directory_name_you_like>` should be
```
<directory_name_you_like>
|-- captions
    |-- cap.dress.val.json
    |-- cap.shirt.val.json
    |-- cap.toptee.val.json
| -- image_splits
| -- images
```
The directory `FashionIQ_multi_opt_gpt35_5` is an example of such a directory. The json files in its `captions` directory are obtained by using GPT-3.5 turbo and 5 multi captions in steps 1 and 2. 

To do Step 3, run
```
./run_sem.sh
```
In the shell script, the `--dataset_path` argument should be `<directory_name_you_like>`.

## 2025/05/28

### Update
- Step 1:
    - Change to use BLIP2_MODEL opt6.7b (reacll ⭡)
    - Change to NUM_CAPTION 15 (stpe 2 spend time ⭡, reacll unkown)

- Step 2:
    - code modify input gpt prompt (reacll ⭡)

- Step 3:  
    - Modify get ref_name_list in step 3 code (reacll ⭡)
    - Add original input caption features (reacll ⭡)


### Code Explanation
    Step 2:

    LLM-based_editing_reasoner_fashioniq_v1.py -> use gpt version
    LLM-based_editing_reasoner_fashioniq.py -> use LLama2-7b version

    Step 3:
    semantic_editing_search_v1.py -> modify version

### Captions store
The captions output use BLIP2_MODEL opt6.7b run after step 1 
```
LLM-edit
| -- captions_num_5
| -- captions_num_15
| -- edit_5
```

### Captions Directory for step 3
I put the all version caption data in FashionIQ_multi_opt_gpt35_5 directory
```
srcFashionIQ_multi_opt_gpt35_5
| -- captions
    |-- cap.dress.val.json
    |-- cap.shirt.val.json
    |-- cap.toptee.val.json
| -- caption_gpt3.5_v1 -> gpt 
| -- caption_Llama2 -> Llama2-7b
| -- image_splits
| -- images
```

### Ablation Study Results (FashionIQ Recall)

#### LLM: LLama2-7B
| Category | Model | pos_factor | neg_factor | add_original| Recall@10 (%) | Recall@50(%) |
|----|---|------|---|---|--------|--------|
| Shirt | SEIZE-g | 0.5 | 0 | ✔️ | 33.32 | 52.60 |
| Dress | SEIZE-g | 0.5 | 0 | ✔️ | 28.06 | 50.27 |
| Toptee | SEIZE-g | 0.5 | 0 | ✔️ | 37.48 | 59.15 |
| **Average** | | | | **32.95** | **54.01** |
| | | | | | | |
| Shirt | SEIZE-G | 0.5 | 0 | ✔️ | 34.59 | 52.75 |
| Dress | SEIZE-G | 0.5 | 0 | ✔️ | 26.82 | 48.83 |
| Toptee | SEIZE-G | 0.5 | 0 | ✔️ | 37.43 | 60.22 |
| **Average** | | | | **32.95** | **53.94** |
| | | | | | | |
| Shirt | SEIZE-H | 0.5 | 0 | ✔️ | 32.58 | 51.28 |
| Dress | SEIZE-H | 0.5 | 0 | ✔️ | 27.66 | 49.43 |
| Toptee | SEIZE-H | 0.5 | 0 | ✔️ | 35.95 | 60.28 |
| **Average** | | | | **32.07** | **53.66** |
| | | | | | | |
| Shirt | SEIZE-H | 0.5 | 0 | ✔️ | 32.58 | 51.28 |
| Dress | SEIZE-H | 0.5 | 0 | ✔️ | 27.66 | 49.43 |
| Toptee | SEIZE-H | 0.5 | 0 | ✔️ | 35.95 | 60.28 |
| **Average** | | | | **32.07** | **53.66** |
| | | | | | | |
| Shirt | SEIZE-L | 0.5 | 0 | ❌ | 25.42 | 41.12 |
| Dress | SEIZE-L | 0.5 | 0 | ❌ | 18.00 | 39.12 |
| Toptee | SEIZE-L | 0.5 | 0 | ❌ | 25.04 | 47.12 |
| **Average** | | | | **22.82** | **42.45** |
| | | | | | | |
| Shirt | SEIZE-g | default | default | ❌ | 33.91 | 51.37|
| Dress | SEIZE-g | default | default | ❌ | 26.62 | 47.00 |
| Toptee | SEIZE-g | default | default | ❌ | 34.83 | 56.40 |
| **Average** | | | | **31.79** | **51.59** |


#### LLM: GPT3.5
| Category | Model | pos_factor | neg_factor | add_original| Recall@10 (%) | Recall@50(%) |
|----|---|------|---|---|--------|--------|
| Shirt | SEIZE-g | 0.5 | 0 | ❌ |  |  |
| Dress | SEIZE-g | 0.5 | 0 | ❌ | 31.38 | 53.74 |
| Toptee | SEIZE-g | 0.5 | 0 | ❌ |  |  |
| **Average** | | | | |  |
| | | | | | | |
| Shirt | SEIZE-g | 0.5 | 0 | ✔️ |  |  |
| Dress | SEIZE-g | 0.5 | 0 | ✔️ | 30.64 | 52.80 |
| Toptee | SEIZE-g | 0.5 | 0 | ✔️ |  |  |
| **Average** | | | | |  |
| | | | | | | |
| Shirt | SEIZE-g | default | default  | ❌ |  |  |
| Dress | SEIZE-g | default  | default  | ❌ | 30.59 | 53.40 |
| Toptee | SEIZE-g | default  | default  | ❌ |  |  |


reference : https://github.com/yzy-bupt/SEIZE

## 2025/06/03
### Updates
In `src/semantic_editing_search_v1.py`, 
- I added a `use_cache=False` argument in `fiq_compute_val_metrics` function so that it does not load the cached `predicted_features`, `original_features`, `target_names`, etc but recompute them instead. This is to prevent the possiblity that the cache is wrong. For example, I am computing `predicted_features` from a relative dataset of the same dress type and model type, but the dataset uses GPT outputs from prompt 2, whereas the cache stores predicted_features from GPT outputs from prompt 1. Then the cached `predicted_features` should not be used here. Thus, it is recommended to set `use_cache=False`.

- The line `os.environ['CUDA_VISIBLE_DEVICES'] = '1'` is commented out.

- The variables `distances` and `sorted_index_names` are saved. These variables are later needed for visualization. So far, they are stored in the directory `retrieved_index_names`, but the save location is subject to change. 

In `src/utils.py`,
- Added `sample_dataset` to create a sample of dataset json. This is useful when one wants to test something with the dataset quickly but does not want to wait for running through the whole dataset. 

### TODO
Visualization pipeline that will take in the saved `distances` and `sorted_index_names`.

